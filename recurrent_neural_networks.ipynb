{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural network is a type of network architecture that accepts variable inputs and variable outputs, which contrasts with the vanilla feed-forward neural networks. \n",
    "\n",
    "We can also consider input with variable length, such as video frames and we want to make a decision along every frame of that video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Process Sequences\n",
    "![sequence](img/sequence.png)\n",
    "\n",
    "#### one to one\n",
    "This is the classic feed forward neural network architecture, with one input and we expect one output.\n",
    "\n",
    "#### one to many\n",
    "This can be thought of as image captioning. We have one image as a fixed size input and the output can be words or sentences which are variable in length.\n",
    "\n",
    "#### many to one\n",
    "This is used for sentiment classification. The input is expected to be a sequence of words or even paragraphs of words. The output can be a regression output with continuous values which represent the likelihood of having a positive sentiment.\n",
    "\n",
    "#### many to many\n",
    "This model is ideal for machine translation like the one we see on Google translate. The input could an English sentence which has variable length and the output will be the same sentence in a different language which also has variable length. \n",
    "\n",
    "The last many to many model can be used for video classification on frame level. Feed every frame of a video into the neural network and expect an output right away. However, since frames are generally dependent on each other, it is necessary for the network to propagate its hidden state from the previous to the next. Thus, we need recurrent neural network for this kind of task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Mathematical Formulation\n",
    "We can process a sequence of vectors **x** applying a recurrence formula at every time step:\n",
    "\n",
    "$$\n",
    "h_{t} = f_{W}(h_{t - 1}, x_{t})\n",
    "$$\n",
    "\n",
    "`x[t]` is input vector at some time step while `h[t - 1]` is the previous hidden state and `h[t]` is the new hidden state produced by the network. The whole forward propagation can be characterized by the function of **W**.\n",
    "\n",
    "**NOTE**: The same function and same set of parameters are used at every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example\n",
    "Here's a simple vanilla recurrent neural network example in functional form. Notice that if we were to produce `h[t]`, we need some weight matrices, `h[t-1]`, `x[t]` and a non-linearity `tanh`.\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x_{t})\n",
    "$$\n",
    "\n",
    "If we want to produce an output at every timestep, then we want another weight matrix that accepts a hidden state and project it to an output `y`\n",
    "\n",
    "$$\n",
    "y_{t} = W_{hy}h_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "class RecurrentNet(object):\n",
    "    \"\"\"When we say W_hh, it means a weight matrix that accepts a hidden state and produce a new hidden state. \n",
    "    Similarly, W_xh represents a weight matrix that accepts an input vector and produce a new hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.hidden_state = np.zeros((3, 3))\n",
    "        self.W_hh = np.random.randn(3, 3)\n",
    "        self.W_xh = np.random.randn(3, 3)\n",
    "        self.W_hy = np.random.randn(3, 3)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        self.hidden_state = np.tanh(self.W_hh.dot(self.hidden_state) + self.W_xh.dot(x))\n",
    "        \n",
    "        return self.W_hy.dot(self.hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.80121891 -2.80121891 -2.80121891]\n",
      " [ 0.69469708  0.69469708  0.69469708]\n",
      " [ 0.96886448  0.96886448  0.96886448]]\n",
      "[[-3.04406657 -3.04406657 -3.04406657]\n",
      " [ 0.7898616   0.7898616   0.7898616 ]\n",
      " [ 0.86068273  0.86068273  0.86068273]]\n",
      "[[-3.04498651 -3.04498651 -3.04498651]\n",
      " [ 0.78981591  0.78981591  0.78981591]\n",
      " [ 0.86049092  0.86049092  0.86049092]]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.ones((3, 3))\n",
    "silly_network = RecurrentNet()\n",
    "\n",
    "# Notice that same input, but leads to different ouptut at every single time step.\n",
    "print silly_network.forward_prop(input_vector)\n",
    "print silly_network.forward_prop(input_vector)\n",
    "print silly_network.forward_prop(input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Computational Graph\n",
    "Instead of imagining that hidden state is being *recurrently* fed back into the network, it's easier to visualize the process if we unroll the operation into a computational graph that is composed to many time steps.\n",
    "\n",
    "For example, we begin with a zero'ed vector as our hidden state on the left. We feed it into the network along with our first input. When we receive the next input, we take the new hidden state and feed it into the network again with the second input. The procoess goes on until the point we wish to compute the final output of the network.\n",
    "\n",
    "![computational-graph-1](img/computational-graph-1.png)\n",
    "\n",
    "We use the same set of weight for every time step of the computation.\n",
    "\n",
    "![computational-graph-2](img/computational-graph-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Computational Graph - Many to Many\n",
    "For the many-to-many case, we compute a `y[t]` and the loss for every time step. At the end we simply sum up the loss of all the time steps and count that as our total loss of the network. \n",
    "\n",
    "When we think about the back propagation for this model, we will have a separate gradient for W flowing from each of those time steps and then the final gradient for W will be the sum of all those individual time step gradients. *Imagine that we have some sort of ground-truth label for every step of the sequence*:\n",
    "\n",
    "\n",
    "![computational-graph-many-to-many](img/computational-graph-many-to-many.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Computational Graph - Many to One\n",
    "If we have this many to one situation, we make the decision based on the final hidden state of this network. This final hidden state summarizes all of the context from the entire sequence. \n",
    "\n",
    "\n",
    "![computational-graph-many-to-one](img/computational-graph-many-to-one.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Computational Graph - One to Many\n",
    "If we have this one to many situation, where we want to receive a fixed size input and produce a variable length output, then you would commonly use that fixed size input to initialize the hidden state and then let the network to propagate and evolve the hidden state forward. \n",
    "\n",
    "\n",
    "![computational-graph-one-to-many](img/computational-graph-one-to-many.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Squence to Sequence\n",
    "For the sequence to sequence models where you might want to do something like machine translation, this is a combination of **many-to-one** and **one-to-many** architecture. We proceed in two stages, (1) the encoder receives a variably sized input like an english sentence and performs encoding into a hidden state vector, (2) the decoder receives the hidden state vector and produces a variably sized output. The motivation of using this architecture is modularity. We can easily swap out encoder and decoder for different type of language translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Character-leve Languag Model\n",
    "### Training Time\n",
    "Suppose that we have a character-level language model, the list of possible *vocabularies* is `['h', 'e', 'l', 'o']`.  An example training sequence is `hello`. The same output from hidden layer is being fed to output layer and the next hidden layer, as noted below that `y[t]` is a product of `W_hy` and `h[t]`. Since we know what we are expecting, we can backpropagate the cost and update weights.\n",
    "\n",
    "The `y[t]` is a prediction for which letter is most likely to come next. For example, when we feed `h` into the network, `e` is the expected output of the network because the only training example we have is `hello`. \n",
    "\n",
    "![language-model](img/language-model.png)\n",
    "\n",
    "### Test Time\n",
    "At test time, we sample characters one at a time and feed it back to the model to produce a whole sequence of characters (which makes up a word.) We seed the word with a prefix like the letter **h** in this case. The output is a softmax vector which represents probability. We can use it as a probability distribution and perform sampling.\n",
    "\n",
    "**This means EACH character has some chance to be selected** Samplng technique gives us more diversity in the output. This is evident in sentence construction. Given a prefix, we can have multiple words and phrases to represent the same idea.\n",
    "\n",
    "![language-model-test-time](img/language-model-test-time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Backpropagation\n",
    "The concept of backpropagation through time is that we forward through entire sequence to compute loss, then backward through entire sequence to compute gradient. However, this becomes problematic when we want to train a sequence that is very long. For example, if we were to train a a paragraph of words, we have to iterate through many layers before we can compute one simple gradient step.\n",
    "\n",
    "### Truncated Backprop\n",
    "In practice, what people do is an approximation called truncated backpropagation through time. Run forward and backward through chunks of the sequence instead of the whole sequence. Even though our input sequence can potentially be very long or even infinite, when we are training our model, we will step forward for some number of steps and compute a loss only over this sub sequence of the data. Then backpropagate through this sub-sequence and make a gradient step on the weights. \n",
    "\n",
    "When we move to the next batch, we still have this hidden state from the previous batch of data, we will carry this hidden state forward. The forward pass is unaffected but we will only backpropgate again through this second batch. \n",
    "\n",
    "![truncated-backprop](img/truncated-backprop.png)\n",
    "\n",
    "###  Example: Minimal character-level Vanilla RNN model\n",
    "Let's use the same `tanh` example we had up there to implement a single layer recurrent nerual network. The forward pass is quite easy. Assuming the input is a list of character index, i.e. `a => 0`, `b => 1`, etc..., the target is a list of character index that represents the next letter in the sequence. For example, the target is characters of the word `ensorflow` and the input is `tensorflo`. Given a letter `t`, it should predict that next letter is `e`. \n",
    "\n",
    "```python\n",
    "# Encode input state in 1-of-k representation\n",
    "input_states[t] = np.zeros((self.input_dim, 1))\n",
    "input_states[t][input_list[t]] = 1\n",
    "\n",
    "# Compute hidden state\n",
    "hidden_states[t] = tanh(dot(self.params['Wxh'], input_states[t]) +\n",
    "                        dot(self.params['Whh'], hidden_states[t-1]) +\n",
    "                        self.params['Bh'])\n",
    "\n",
    "# Compute output state a.k.a. unnomralized log probability using current hidden state\n",
    "output_states[t] = dot(self.params['Why'], hidden_states[t]) + self.params['By']\n",
    "\n",
    "# Compute softmax probability state using the output state\n",
    "prob_states[t] = exp(output_states[t]) / np.sum(exp(output_states[t]))\n",
    "```\n",
    "\n",
    "Now here's the fun part, computing the gradients for backpropagation. First of all, let's remind ourself what our model is.\n",
    "\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x_{t}) + B_{h}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{t} = W_{hy}h_{t} + B_{y}\n",
    "$$\n",
    "\n",
    "First compute the gradient of loss with respect to output vector `y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{t}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "# Softmax gradient\n",
    "grad_output = np.copy(prob_states[t])\n",
    "grad_output[target_list[t]] -= 1\n",
    "```\n",
    "\n",
    "Then gradient of loss with respect to `Why`, `h`, and the bias:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{hy}} = \\frac{\\partial L}{\\partial y_{t}} \\cdot \\frac{\\partial y_{t}}{\\partial W_{hy}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B_{y}} = \\frac{\\partial L}{\\partial y_{t}} \\cdot \\frac{\\partial y_{t}}{\\partial B_{y}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t}} = \\frac{\\partial L}{\\partial y_{t}} \\cdot \\frac{\\partial y_{t}}{\\partial h_{t}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "grads['Why'] += dot(grad_output, hidden_states[t].T)\n",
    "grads['By'] += grad_output\n",
    "grad_h = dot(self.params['Why'].T, grad_output) + grad_prev_h # (H, O)(O, H) => (H, H)\n",
    "```\n",
    "\n",
    "We need to perform a little u-substitution here to simplify our derivatives. \n",
    "\n",
    "$$\n",
    "h_{t} = tanh(u) + B_{h}\n",
    "$$\n",
    "\n",
    "So we find the gradient of loss with respect to `u` and then use that to find rest of the gradients. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u} = \\frac{\\partial L}{\\partial h_{t}} \\cdot \\frac{\\partial h_{t}}{\\partial u}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B_{h}} = \\frac{\\partial L}{\\partial h_{t}} \\cdot \\frac{\\partial h_{t}}{\\partial B_{h}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "grad_u = (1 - hidden_states[t] * hidden_states[t]) * grad_h\n",
    "grads['Bh'] += grad_u\n",
    "```\n",
    "\n",
    "Finally, we can compute the gradients for the last two parameters:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{xh}} = \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial W_{xh}}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial W_{hh}} = \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial W_{hh}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial h_{t-1}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "grads['Wxh'] += dot(grad_u, input_states[t].T)\n",
    "grads['Whh'] += dot(grad_u, hidden_states[t-1].T)\n",
    "grad_prev_h = dot(self.params['Whh'].T, grad_u)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text document contains 727 characters and has 40 unique characters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from adagrad import AdaGradOptimizer\n",
    "from data_util import *\n",
    "from vanilla_rnn import VanillaRNNModel\n",
    "\n",
    "hidden_dim = 100\n",
    "seq_length = 50\n",
    "learning_rate = 1e-1\n",
    "text_data, char_to_idx, idx_to_char = load_dictionary(\"datasets/word_dictionary.txt\")\n",
    "model = VanillaRNNModel(len(char_to_idx), hidden_dim)\n",
    "optimizer = AdaGradOptimizer(model, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curr_iter, pointer, epoch_size, total_iters = 0, 0, 100, 20000\n",
    "\n",
    "steps, losses = [], []\n",
    "while curr_iter < total_iters:\n",
    "    if curr_iter == 0 or pointer + seq_length + 1 >= len(text_data):\n",
    "        prev_hidden_state = np.zeros((hidden_dim, 1))  # Reset RNN memory\n",
    "        pointer = 0  # Reset the pointer\n",
    "    \n",
    "    # Since we are trying to predict next letter in the sequence, the target is simply pointer + 1\n",
    "    input_list = [char_to_idx[ch] for ch in text_data[pointer:pointer+seq_length]]\n",
    "    target_list = [char_to_idx[ch] for ch in text_data[pointer+1: pointer+seq_length+1]]\n",
    "    loss, grads, prev_hidden_state = model.loss(input_list, target_list, prev_hidden_state)\n",
    "    if curr_iter % epoch_size == 0:\n",
    "        steps.append(curr_iter)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    optimizer.update_param(grads)\n",
    "    curr_iter += 1 \n",
    "    pointer += seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb32a667610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(steps, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "\n",
      "Harow y,\n",
      "I doubted if I thy\n",
      "I sng ales fir wig hand woodt pavel caat the pa thad wook.\n",
      "I toubass her way lelding the ing paver come back.\n",
      "I shablent I coow wim,\n",
      "And ood wook ther, agep helllabloth hen trok the per dias just as fair,\n",
      "And having pack.\n",
      "Oh, I kept the passingwthads far as I could\n",
      "Tn the und rorged weand I-\n",
      "I took the one lesselodhi;\n",
      "Theter, lent ond bot tellae that as aod\n",
      "Thoudaigh\n",
      " ood yyinghlorhen black.\n",
      "Oh, I kept the first f or,\n",
      "And both them roallars waI st the utrasther clally In took the other, agel asarn tooked hasthads oo that tr woodelecergrl thad wir;\n",
      "The waes I coued and I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I cook the same,\n",
      "And by louthe on that the undimgwleiescergrl Iesthererowth;\n",
      "Then togn to wing there in w wood, and I-\n",
      "I took the one lesseloksss rn to,\n",
      "And that hasse\n",
      "Had wor sngn the passing hand that tre und ine ped ages hen tey and tod,\n",
      "And both thelethergvlotdiAgvene anit  ver wlock.\n",
      "Though as for that\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# Pick a random character and sample a 100 characters long sequence (i.e. a sentence.)\n",
    "letter = 'H'\n",
    "hidden_state = np.zeros_like((hidden_dim, 1))\n",
    "_, sampled_indices = model.sample_chars(prev_hidden_state, char_to_idx[letter], 1000)\n",
    "predicted_text = ''.join(idx_to_char[idx] for idx in sampled_indices)\n",
    "print \"-------------\\n%s\\n-------------\" % predicted_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
