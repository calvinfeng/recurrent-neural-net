{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short-term Memory)\n",
    "\n",
    "## Multi-layer RNN\n",
    "Stacking layers of RNN together is simply taking the output hidden state and feed it into another hidden layer as an input sequence.\n",
    "\n",
    "![multi-layer-rnn](img/multi-layer-rnn.png)\n",
    "\n",
    "$$\n",
    "h^{layer}_{t} = tanh \\begin{pmatrix} W^{layer} \\begin{pmatrix} h^{layer - 1}_{t} \\\\ h^{layer}_{t-1} \\end{pmatrix} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In this case, the `W[l]` is a `(hidden_dim, 2 * hidden_dim)` matrix.\n",
    "\n",
    "## Exploding Gradient in Vanilla RNN\n",
    "### Gradient Flow\n",
    "Recall that our RNN model:\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh} h_{t-1} + W_{xh}x_{t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{t} = tanh \\begin{pmatrix} (W_{hh} W_{xh}) \\begin{pmatrix} h_{t-1} \\\\ x_{t} \\end{pmatrix} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{t} = tanh \\begin{pmatrix} W \\begin{pmatrix} h_{t-1} \\\\ x_{t} \\end{pmatrix} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In every time step of a sequence, we backprogate from `h[t]` to `h[t-1]`. First the gradient will flow through the `tanh` gate and then to matrix multiplication gate. As we know, whenever we backprop into matrix multiplication gate, the upstream gradient is multiplied by the tranpose of the `W` matrix. This happens at every time step throughout the sequence. What if the sequence is very long?\n",
    "\n",
    "![rnn-gradient-flow](img/rnn-gradient-flow.png)\n",
    "\n",
    "The final expression for gradient on `h[0]` will involve many factors of this weight matrix. This will either lead to an exploding gradient problem or vanishing gradient problem. There' a simple hack to address this problme, which is using `numpy.clip`. However, if the problem is vanishing gradient, clipping isn't going to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing LSTM\n",
    "\n",
    "LSTM has a fancier recurrence relation than the vanilla RNN. LSTM has two states, one is being the usual hidden state `h[t]` we see in vanilla RNN and another one is called the cell state `c[t]`. Cell state is an internal vector that is not exposed to the outside world. \n",
    "\n",
    "Let's define some terminologies here: \n",
    "\n",
    "* `f` **forget gate**: whether to erase cell\n",
    "* `i` **input gate**: whether to write to cell\n",
    "* `g` **gate gate**: how much to write to cell\n",
    "* `o` **output gate**: how much to reveal cell\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} i \\\\ f \\\\ o \\\\ g \\end{pmatrix} = \\begin{pmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ tanh \\end{pmatrix}\n",
    "W \\begin{pmatrix} h_{t - 1} \\\\ x_{t} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Note** that the sigma symbol represents sigmoid activation function.\n",
    "\n",
    "$$\n",
    "c_{t} = f \\odot c_{t - 1} + i \\odot g\n",
    "$$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$\n",
    "c_{t} = \\sigma(W_{hhf} h_{t-1} + W_{xhf} x_{t}) \\odot c_{t-1} + \\sigma(W_{hhi} h_{t-1} + W_{xhi} x_{t}) \\odot tanh(W_{hhg} h_{t-1} + W_{xhg} x_{t})\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "h_{t} = o \\odot tanh(c_{t})\n",
    "$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$\n",
    "h_{t} = \\sigma \\begin{pmatrix} W_{hho} h_{t-1} + W_{xho} x_{t} \\end{pmatrix} \\odot tanh(c_{t})\n",
    "$$\n",
    "\n",
    "We take the previous cell state and hidden state as the inputs to our LSTM cell. The previous hidden state is combined with the input vector and multiply with the weight matrix to produce `ifog`. The forget gate multiplies element-wise with the previous cell state. The input and gate gate also multiply element wise. The two results are combined through sum elemenwise to produce a new cell state. The cell state is then squashed by a `tanh` and multiplied element-wise by the output gate to produce our next hidden state.\n",
    "![lstm](img/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Gradient Flow\n",
    "Backpropagating from `c[t]` to `c[t-1]` is only element-wise multiplication by the `f` gate, and there is no matrix multiplication by W. The `f` gate is different at every time step, ranged between 0 and 1 due to sigmoid property, thus we have avoided of the problem of multiplying the same thing over and over again. \n",
    "\n",
    "Backpropagating from `h[t]` to `h[t-1]` is going through only one single `tanh` nonlinearity rather than `tanh` for every single step.\n",
    "\n",
    "![cell-state-gradient-flow](img/cell-state-gradient-flow.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
